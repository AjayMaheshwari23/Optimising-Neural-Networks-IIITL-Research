{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlE_jisTkXY4"
   },
   "source": [
    "**Copyright 2021 The TensorFlow Authors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:22.994100Z",
     "iopub.status.busy": "2024-03-09T12:49:22.993539Z",
     "iopub.status.idle": "2024-03-09T12:49:22.997711Z",
     "shell.execute_reply": "2024-03-09T12:49:22.997064Z"
    },
    "id": "mEE8NFIMSGO-"
   },
   "outputs": [],
   "source": [
    "# The Ultimate compression Pipeline  ~ Ajay Maheshwari ( LCI2021023 ) under Dr. Mainak Adhikari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J63wSeDoZZwd"
   },
   "source": [
    "# Step 1.  Creating our Base Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:23.001543Z",
     "iopub.status.busy": "2024-03-09T12:49:23.001064Z",
     "iopub.status.idle": "2024-03-09T12:49:25.431792Z",
     "shell.execute_reply": "2024-03-09T12:49:25.430649Z"
    },
    "id": "3asgXMqnSGPE"
   },
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:25.436272Z",
     "iopub.status.busy": "2024-03-09T12:49:25.435968Z",
     "iopub.status.idle": "2024-03-09T12:49:28.313563Z",
     "shell.execute_reply": "2024-03-09T12:49:28.312649Z"
    },
    "id": "gL6JiLXkSGPI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(model):\n",
    "  # Save the model to a temporary file\n",
    "  with tempfile.NamedTemporaryFile(suffix=\".h5\") as temp_file:  # Adjust suffix based on model format\n",
    "    model.save(temp_file.name)\n",
    "\n",
    "    # Create a zip archive and write the temporary file\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "      f.write(temp_file.name)\n",
    "\n",
    "    return os.path.getsize(zipped_file) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:28.318406Z",
     "iopub.status.busy": "2024-03-09T12:49:28.317612Z",
     "iopub.status.idle": "2024-03-09T12:50:51.327361Z",
     "shell.execute_reply": "2024-03-09T12:50:51.326512Z"
    },
    "id": "w7Fd6jZ7SGPL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset + Model Creation \n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images  = test_images / 255.0\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(4, 4), activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(5, 5), activation='tanh', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='tanh', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='selu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='selu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),  # Dropout for regularization\n",
    "    \n",
    "    keras.layers.Dense(128, activation='tanh'),\n",
    "    keras.layers.Dropout(0.4),  # Dropout for regularization\n",
    "    \n",
    "    keras.layers.Dense(64, activation='selu'),\n",
    "    keras.layers.Dropout(0.4),  # Dropout for regularization\n",
    "    \n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1688/1688 [==============================] - 99s 58ms/step - loss: 0.3376 - accuracy: 0.9086 - val_loss: 0.0940 - val_accuracy: 0.9813\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 96s 57ms/step - loss: 0.1282 - accuracy: 0.9703 - val_loss: 0.1071 - val_accuracy: 0.9797\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 133s 79ms/step - loss: 0.1111 - accuracy: 0.9748 - val_loss: 0.1098 - val_accuracy: 0.9747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x282fef490>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model ( 10 Epochs for now )\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_split=0.1,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "#     keras.layers.RandomRotation(0.2),\n",
    "#     keras.layers.RandomZoom(0.2),\n",
    "# #     layers.RandomContrast(0.2),\n",
    "# #     layers.Lambda(lambda x: tf.image.random_brightness(x, max_delta=0.2)),  # Adding random brightness\n",
    "# #     layers.Lambda(lambda x: tf.image.random_saturation(x, lower=0.8, upper=1.2)),  # Adding random saturation\n",
    "# #     layers.Lambda(lambda x: tf.image.random_hue(x, max_delta=0.2)),  # Adding random hue\n",
    "# #     layers.Lambda(lambda x: tf.image.random_jpeg_quality(x, min_jpeg_quality=80, max_jpeg_quality=100))  # Random JPEG quality\n",
    "# ])\n",
    "\n",
    "# # Apply data augmentation to the test images\n",
    "# augmented_images = data_augmentation(test_images, training=False)\n",
    "\n",
    "# # Combine the original and augmented test images and labels\n",
    "# augmented_test_images = np.concatenate([test_images, augmented_images], axis=0)\n",
    "# augmented_test_labels = np.concatenate([test_labels, test_labels], axis=0)  # Duplicate labels for augmented images\n",
    "\n",
    "# print('Original test images shape:', test_images.shape)\n",
    "# print('Augmented test images shape:', augmented_test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBOQ8MeESGPO"
   },
   "source": [
    "### Evaluating the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:51.331343Z",
     "iopub.status.busy": "2024-03-09T12:50:51.330742Z",
     "iopub.status.idle": "2024-03-09T12:50:51.987721Z",
     "shell.execute_reply": "2024-03-09T12:50:51.986958Z"
    },
    "id": "HYulekocSGPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model test accuracy: 0.9704999923706055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model size:  6960.145  KB\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Base Model test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "print(\"Base model size: \",  get_gzipped_model_size(model) , ' KB' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize dictionary to store weights and their counts\n",
    "# weight_counts = {}\n",
    "\n",
    "# # Iterate through layers\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, 'get_weights'):\n",
    "#         layer_weights = layer.get_weights()\n",
    "#         for w in layer_weights:\n",
    "#             # Flatten weights if necessary\n",
    "#             w_flat = w.flatten()\n",
    "#             # Update dictionary\n",
    "#             for weight in w_flat:\n",
    "#                 if weight in weight_counts:\n",
    "#                     weight_counts[weight] += 1\n",
    "#                 else:\n",
    "#                     weight_counts[weight] = 1\n",
    "\n",
    "# # Extract unique weights and counts\n",
    "# unique_weights = np.array(list(weight_counts.keys()))\n",
    "# counts = np.array(list(weight_counts.values()))\n",
    "\n",
    "# # Plot the graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(unique_weights, counts, width=0.1)\n",
    "# plt.xlabel('Weights')\n",
    "# plt.ylabel('Number of Connections')\n",
    "# plt.title('Weights vs Number of Connections')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPoCr4OFkXZE"
   },
   "source": [
    "# Step 2. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity_values = []\n",
    "# accuracies = []\n",
    "\n",
    "# for sparsity in np.arange(0.1, 1.0, 0.1):\n",
    "#     prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "#     pruning_params = {\n",
    "#           'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(sparsity, begin_step=0, frequency=100)\n",
    "#       }\n",
    "\n",
    "#     callbacks = [\n",
    "#       tfmot.sparsity.keras.UpdatePruningStep()\n",
    "#     ]\n",
    "\n",
    "#     pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "#     opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "#     pruned_model.compile(\n",
    "#       loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#       optimizer=opt,\n",
    "#       metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#     pruned_model.fit(\n",
    "#       train_images,\n",
    "#       train_labels,\n",
    "#       epochs=3,\n",
    "#       validation_split=0.1,\n",
    "#       callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "#     _, accuracy = pruned_model.evaluate(test_images, test_labels, verbose=0)\n",
    "    \n",
    "\n",
    "#     sparsity_values.append(sparsity)\n",
    "#     accuracies.append(accuracy)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(sparsity_values, accuracies, marker='o')\n",
    "# plt.xlabel('Sparsity')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy vs Sparsity')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:51.992256Z",
     "iopub.status.busy": "2024-03-09T12:50:51.991693Z",
     "iopub.status.idle": "2024-03-09T12:50:52.943566Z",
     "shell.execute_reply": "2024-03-09T12:50:52.942753Z"
    },
    "id": "mqsN5tP-kXZF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Gnerally it starts dropping for pruning > 60% connections\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.7, begin_step=0, frequency=100)\n",
    "  }\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "pruned_model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:52.948105Z",
     "iopub.status.busy": "2024-03-09T12:50:52.947434Z",
     "iopub.status.idle": "2024-03-09T12:51:17.402820Z",
     "shell.execute_reply": "2024-03-09T12:51:17.401918Z"
    },
    "id": "2aBxR8uEkXZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1688/1688 [==============================] - 172s 99ms/step - loss: 0.2834 - accuracy: 0.9570 - val_loss: 0.0959 - val_accuracy: 0.9858\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 116s 69ms/step - loss: 0.2069 - accuracy: 0.9726 - val_loss: 0.0704 - val_accuracy: 0.9872\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 166s 98ms/step - loss: 0.1605 - accuracy: 0.9793 - val_loss: 0.0573 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2929adf10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-training Model so that remaining connections learn again \n",
    "\n",
    "pruned_model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=3,\n",
    "  validation_split=0.1,\n",
    "  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, pruned_model_accuracy = pruned_model.evaluate(\n",
    "#     test_images, test_labels, verbose=0)\n",
    "\n",
    "# print('Pruned Model test accuracy:', pruned_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:17.406749Z",
     "iopub.status.busy": "2024-03-09T12:51:17.406047Z",
     "iopub.status.idle": "2024-03-09T12:51:17.412875Z",
     "shell.execute_reply": "2024-03-09T12:51:17.412200Z"
    },
    "id": "XL-zWoU4kXZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZRAJVqWkXZG"
   },
   "source": [
    "### Checking if actually pruned or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:17.416380Z",
     "iopub.status.busy": "2024-03-09T12:51:17.415788Z",
     "iopub.status.idle": "2024-03-09T12:51:17.453872Z",
     "shell.execute_reply": "2024-03-09T12:51:17.453121Z"
    },
    "id": "_8_p--1NkXZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_10/kernel:0: 69.97% sparsity  (403/576)\n",
      "conv2d_11/kernel:0: 70.00% sparsity  (22938/32768)\n",
      "conv2d_12/kernel:0: 70.00% sparsity  (35840/51200)\n",
      "conv2d_13/kernel:0: 70.00% sparsity  (51610/73728)\n",
      "conv2d_14/kernel:0: 70.00% sparsity  (103219/147456)\n",
      "conv2d_15/kernel:0: 70.00% sparsity  (103219/147456)\n",
      "dense_11/kernel:0: 70.00% sparsity  (91750/131072)\n",
      "dense_12/kernel:0: 70.00% sparsity  (22938/32768)\n",
      "dense_13/kernel:0: 70.00% sparsity  (5734/8192)\n",
      "dense_14/kernel:0: 70.00% sparsity  (448/640)\n"
     ]
    }
   ],
   "source": [
    "def print_model_weights_sparsity(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )\n",
    "\n",
    "\n",
    "stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "print_model_weights_sparsity(stripped_pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_pruned_model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# stripped_pruned_model.fit(train_images,\n",
    "#   train_labels,\n",
    "#   epochs=3,\n",
    "#   validation_split=0.1,\n",
    "#   callbacks=callbacks)\n",
    "\n",
    "# _, pruned_model_accuracy = stripped_pruned_model.evaluate(\n",
    "#     test_images, test_labels, verbose=0)\n",
    "\n",
    "# print('Pruned Model test accuracy:', pruned_model_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model test accuracy: 0.9882000088691711\n",
      "Pruned model size:  5588.956  KB\n"
     ]
    }
   ],
   "source": [
    "_, pruned_model_accuracy = stripped_pruned_model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Pruned Model test accuracy:', pruned_model_accuracy)\n",
    "\n",
    "print(\"Pruned model size: \",  get_gzipped_model_size(stripped_pruned_model) , ' KB' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize dictionary to store weights and their counts\n",
    "# weight_counts = {}\n",
    "\n",
    "# # Iterate through layers\n",
    "# for layer in stripped_pruned_model.layers:\n",
    "#     if hasattr(layer, 'get_weights'):\n",
    "#         layer_weights = layer.get_weights()\n",
    "#         for w in layer_weights:\n",
    "#             # Flatten weights if necessary\n",
    "#             w_flat = w.flatten()\n",
    "#             # Update dictionary\n",
    "#             for weight in w_flat:\n",
    "#                 if weight in weight_counts:\n",
    "#                     weight_counts[weight] += 1\n",
    "#                 else:\n",
    "#                     weight_counts[weight] = 1\n",
    "\n",
    "# # Extract unique weights and counts\n",
    "# unique_weights = np.array(list(weight_counts.keys()))\n",
    "# counts = np.array(list(weight_counts.values()))\n",
    "\n",
    "# # Plot the graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(unique_weights, counts, width=0.1)\n",
    "# plt.xlabel('Weights')\n",
    "# plt.ylabel('Number of Connections')\n",
    "# plt.title('Weights vs Number of Connections')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  Step 2 Pruning Done ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWPgcnjKSGPR"
   },
   "source": [
    "# Step 3. Weight Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_weight_clusters(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            # ignore auxiliary quantization weights\n",
    "            if \"quantize_layer\" in weight.name:\n",
    "                continue\n",
    "            if \"kernel\" in weight.name:\n",
    "                unique_count = len(np.unique(weight))\n",
    "                print(\n",
    "                    f\"{layer.name}/{weight.name}: {unique_count} clusters \"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:17.457801Z",
     "iopub.status.busy": "2024-03-09T12:51:17.457165Z",
     "iopub.status.idle": "2024-03-09T12:51:42.585401Z",
     "shell.execute_reply": "2024-03-09T12:51:42.584660Z"
    },
    "id": "RetnGeQnkXZH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sparsity preserving clustering model:\n",
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 161s 93ms/step - loss: 0.0462 - accuracy: 0.9875 - val_loss: 0.0486 - val_accuracy: 0.9880\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 109s 64ms/step - loss: 0.0305 - accuracy: 0.9914 - val_loss: 0.0537 - val_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2854b2310>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.clustering.keras.experimental import (\n",
    "    cluster,\n",
    ")\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "cluster_weights = cluster.cluster_weights\n",
    "\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 8,\n",
    "  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS,\n",
    "  'preserve_sparsity': True\n",
    "}\n",
    "\n",
    "sparsity_clustered_model = cluster_weights(stripped_pruned_model, **clustering_params)\n",
    "\n",
    "sparsity_clustered_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train sparsity preserving clustering model:')\n",
    "sparsity_clustered_model.fit(train_images, train_labels,epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sparsity:\n",
      "\n",
      "kernel:0: 71.88% sparsity  (414/576)\n",
      "kernel:0: 73.65% sparsity  (24134/32768)\n",
      "kernel:0: 75.97% sparsity  (38899/51200)\n",
      "kernel:0: 74.99% sparsity  (55286/73728)\n",
      "kernel:0: 75.52% sparsity  (111354/147456)\n",
      "kernel:0: 75.66% sparsity  (111561/147456)\n",
      "kernel:0: 74.47% sparsity  (97607/131072)\n",
      "kernel:0: 73.85% sparsity  (24199/32768)\n",
      "kernel:0: 72.88% sparsity  (5970/8192)\n",
      "kernel:0: 70.62% sparsity  (452/640)\n",
      "\n",
      "Model clusters:\n",
      "\n",
      "conv2d_10/kernel:0: 8 clusters \n",
      "conv2d_11/kernel:0: 8 clusters \n",
      "conv2d_12/kernel:0: 8 clusters \n",
      "conv2d_13/kernel:0: 8 clusters \n",
      "conv2d_14/kernel:0: 8 clusters \n",
      "conv2d_15/kernel:0: 8 clusters \n",
      "dense_11/kernel:0: 8 clusters \n",
      "dense_12/kernel:0: 8 clusters \n",
      "dense_13/kernel:0: 8 clusters \n",
      "dense_14/kernel:0: 8 clusters \n"
     ]
    }
   ],
   "source": [
    "stripped_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\n",
    "\n",
    "print(\"Model sparsity:\\n\")\n",
    "print_model_weights_sparsity(stripped_clustered_model)\n",
    "\n",
    "print(\"\\nModel clusters:\\n\")\n",
    "print_model_weight_clusters(stripped_clustered_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered Model test accuracy: 0.9872000217437744\n",
      "Clustered model size:  3094.786  KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, sparsity_clustered_model_accuracy = sparsity_clustered_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Clustered Model test accuracy:', sparsity_clustered_model_accuracy)\n",
    "print(\"Clustered model size: \",  get_gzipped_model_size(sparsity_clustered_model) , ' KB' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.) Distillation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import ops\n",
    "import numpy as np\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\"Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            ops.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            ops.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2)\n",
    "\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        keras.layers.Conv2D(28, kernel_size=(5, 5), padding='same'),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(28, kernel_size=(5, 5)),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Conv2D(32, kernel_size=(5, 5), padding='same'),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(32, kernel_size=(5, 5)),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(10),\n",
    "        keras.layers.Activation('softmax')\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 62s 32ms/step - loss: 0.1452 - sparse_categorical_accuracy: 0.9545\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0643 - sparse_categorical_accuracy: 0.9810\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 63s 34ms/step - loss: 0.0548 - sparse_categorical_accuracy: 0.9847\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.030768228694796562, 0.9904999732971191]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.fit(train_images, train_labels, epochs=3)\n",
    "teacher.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "distiller = Distiller(student=stripped_clustered_model, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Pruning + Clustering + Distillation Done .... ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 157s 83ms/step - sparse_categorical_accuracy: 0.8974\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 146s 78ms/step - sparse_categorical_accuracy: 0.9883\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 150s 80ms/step - sparse_categorical_accuracy: 0.9833\n",
      "313/313 [==============================] - 5s 15ms/step - sparse_categorical_accuracy: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9858999848365784"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distill teacher se student\n",
    "distiller.fit(train_images, train_labels, epochs=3)\n",
    "\n",
    "distiller.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfmot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stripped_clustered_model \u001b[38;5;241m=\u001b[39m tfmot\u001b[38;5;241m.\u001b[39mclustering\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mstrip_clustering(distiller\u001b[38;5;241m.\u001b[39mstudent)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel sparsity:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m print_model_weights_sparsity(stripped_clustered_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfmot' is not defined"
     ]
    }
   ],
   "source": [
    "stripped_clustered_model = tfmot.clustering.keras.strip_clustering(distiller.student)\n",
    "\n",
    "print(\"Model sparsity:\\n\")\n",
    "print_model_weights_sparsity(stripped_clustered_model)\n",
    "\n",
    "print(\"\\nModel clusters:\\n\")\n",
    "print_model_weight_clusters(stripped_clustered_model) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31qLY2udZIUc"
   },
   "source": [
    "## Apply QAT and PCQAT and check effect on model clusters and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMO-h8PgZIUc"
   },
   "source": [
    "Next, apply both QAT and PCQAT on the sparse clustered model and observe that PCQAT preserves weight sparsity and clusters in your model. Note that the stripped model is passed to the QAT and PCQAT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:42.651439Z",
     "iopub.status.busy": "2024-03-09T12:51:42.650879Z",
     "iopub.status.idle": "2024-03-09T12:51:55.132299Z",
     "shell.execute_reply": "2024-03-09T12:51:55.131490Z"
    },
    "id": "Nfp-xfHdZIUc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train qat model:\n",
      "422/422 [==============================] - 3s 7ms/step - loss: 0.2440 - accuracy: 0.9518 - val_loss: 0.1033 - val_accuracy: 0.9747\n",
      "Train pcqat model:\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'dense_19/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'dense_19/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'dense_19/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'dense_19/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 4s 7ms/step - loss: 0.3739 - accuracy: 0.9460 - val_loss: 0.1451 - val_accuracy: 0.9682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x16d73e250>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QAT\n",
    "qat_model = tfmot.quantization.keras.quantize_model(stripped_clustered_model)\n",
    "\n",
    "qat_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "print('Train qat model:')\n",
    "qat_model.fit(train_images, train_labels, batch_size=128, epochs=1, validation_split=0.1)\n",
    "\n",
    "# PCQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              stripped_clustered_model)\n",
    "pcqat_model = tfmot.quantization.keras.quantize_apply(\n",
    "              quant_aware_annotate_model,\n",
    "              tfmot.experimental.combine.Default8BitClusterPreserveQuantizeScheme(preserve_sparsity=True))\n",
    "\n",
    "pcqat_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "print('Train pcqat model:')\n",
    "pcqat_model.fit(train_images, train_labels, batch_size=128, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:55.136081Z",
     "iopub.status.busy": "2024-03-09T12:51:55.135494Z",
     "iopub.status.idle": "2024-03-09T12:51:55.145936Z",
     "shell.execute_reply": "2024-03-09T12:51:55.145243Z"
    },
    "id": "6kluyg_2ZIUd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Model clusters:\n",
      "quant_conv2d_28/conv2d_28/kernel:0: 66 clusters \n",
      "quant_dense_19/dense_19/kernel:0: 11763 clusters \n",
      "\n",
      "QAT Model sparsity:\n",
      "conv2d_28/kernel:0: 37.96% sparsity  (41/108)\n",
      "dense_19/kernel:0: 31.06% sparsity  (6298/20280)\n",
      "\n",
      "PCQAT Model clusters:\n",
      "quant_conv2d_28/conv2d_28/kernel:0: 9 clusters \n",
      "quant_dense_19/dense_19/kernel:0: 8 clusters \n",
      "\n",
      "PCQAT Model sparsity:\n",
      "conv2d_28/kernel:0: 70.37% sparsity  (76/108)\n",
      "dense_19/kernel:0: 89.71% sparsity  (18193/20280)\n"
     ]
    }
   ],
   "source": [
    "print(\"QAT Model clusters:\")\n",
    "print_model_weight_clusters(qat_model)\n",
    "print(\"\\nQAT Model sparsity:\")\n",
    "print_model_weights_sparsity(qat_model)\n",
    "print(\"\\nPCQAT Model clusters:\")\n",
    "print_model_weight_clusters(pcqat_model)\n",
    "print(\"\\nPCQAT Model sparsity:\")\n",
    "print_model_weights_sparsity(pcqat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9Ywb9bRZIUd"
   },
   "source": [
    "## See compression benefits of PCQAT model\n",
    "\n",
    "Define helper function to get zipped model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:55.149510Z",
     "iopub.status.busy": "2024-03-09T12:51:55.148960Z",
     "iopub.status.idle": "2024-03-09T12:51:55.153272Z",
     "shell.execute_reply": "2024-03-09T12:51:55.152604Z"
    },
    "id": "vehNHBYsZIUe"
   },
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # It returns the size of the gzipped model in kilobytes.\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBwp4GVmZIUe"
   },
   "source": [
    "Observe that applying sparsity, clustering and PCQAT to a model yields significant compression benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:55.156935Z",
     "iopub.status.busy": "2024-03-09T12:51:55.156392Z",
     "iopub.status.idle": "2024-03-09T12:52:00.340158Z",
     "shell.execute_reply": "2024-03-09T12:52:00.339359Z"
    },
    "id": "mbe2jMAyZIUe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp41ty6m4p/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp41ty6m4p/assets\n",
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1715835084.550317  341577 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1715835084.550831  341577 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-16 10:21:24.551621: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp41ty6m4p\n",
      "2024-05-16 10:21:24.552728: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-16 10:21:24.552733: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp41ty6m4p\n",
      "2024-05-16 10:21:24.562095: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-16 10:21:24.593714: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp41ty6m4p\n",
      "2024-05-16 10:21:24.601537: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 49916 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp8ylysrw7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp8ylysrw7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT model size:  17.169  KB\n",
      "PCQAT model size:  4.578  KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1715835085.429330  341577 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1715835085.429339  341577 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-16 10:21:25.429439: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp8ylysrw7\n",
      "2024-05-16 10:21:25.430709: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-16 10:21:25.430715: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp8ylysrw7\n",
      "2024-05-16 10:21:25.450602: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-16 10:21:25.482693: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmp8ylysrw7\n",
      "2024-05-16 10:21:25.492152: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 62712 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# QAT model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "qat_tflite_model = converter.convert()\n",
    "qat_model_file = 'qat_model.tflite'\n",
    "# Save the model.\n",
    "with open(qat_model_file, 'wb') as f:\n",
    "    f.write(qat_tflite_model)\n",
    "\n",
    "# PCQAT model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pcqat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pcqat_tflite_model = converter.convert()\n",
    "pcqat_model_file = 'pcqat_model.tflite'\n",
    "# Save the model.\n",
    "with open(pcqat_model_file, 'wb') as f:\n",
    "    f.write(pcqat_tflite_model)\n",
    "\n",
    "print(\"QAT model size: \", get_gzipped_model_size(qat_model_file), ' KB')\n",
    "print(\"PCQAT model size: \", get_gzipped_model_size(pcqat_model_file), ' KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WBGBJU3ZIUf"
   },
   "source": [
    "## See the persistence of accuracy from TF to TFLite\n",
    "\n",
    "Define a helper function to evaluate the TFLite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:52:00.343941Z",
     "iopub.status.busy": "2024-03-09T12:52:00.343359Z",
     "iopub.status.idle": "2024-03-09T12:52:00.349610Z",
     "shell.execute_reply": "2024-03-09T12:52:00.348936Z"
    },
    "id": "9P-1dmQcZIUf"
   },
   "outputs": [],
   "source": [
    "def eval_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print(f\"Evaluated on {i} results so far.\")\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "276omav-ZIUf"
   },
   "source": [
    "Evaluate the model, which has been pruned, clustered and quantized, and then see that the accuracy from TensorFlow persists in the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:52:00.353105Z",
     "iopub.status.busy": "2024-03-09T12:52:00.352520Z",
     "iopub.status.idle": "2024-03-09T12:52:01.104976Z",
     "shell.execute_reply": "2024-03-09T12:52:01.104182Z"
    },
    "id": "6p4RBECpZIUg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Pruned + clustered + Distilled quantized TFLite test_accuracy: 0.9594\n",
      "Baseline TF test accuracy: 0.9814000129699707\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(pcqat_model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "pcqat_test_accuracy = eval_model(interpreter)\n",
    "\n",
    "print('Pruned + clustered + Distilled quantized TFLite test_accuracy:', pcqat_test_accuracy)\n",
    "print('Baseline TF test accuracy:', baseline_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQFbw88ykXZL"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JhbpowqSGP1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pcqat_example.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

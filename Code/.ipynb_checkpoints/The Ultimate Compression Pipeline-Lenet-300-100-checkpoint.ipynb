{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyiSRgdtSGPC"
   },
   "source": [
    "   # The Ultimate Compression Pipeline ~ Ajay Maheshwari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:23.001543Z",
     "iopub.status.busy": "2024-03-09T12:49:23.001064Z",
     "iopub.status.idle": "2024-03-09T12:49:25.431792Z",
     "shell.execute_reply": "2024-03-09T12:49:25.430649Z"
    },
    "id": "3asgXMqnSGPE"
   },
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:25.436272Z",
     "iopub.status.busy": "2024-03-09T12:49:25.435968Z",
     "iopub.status.idle": "2024-03-09T12:49:28.313563Z",
     "shell.execute_reply": "2024-03-09T12:49:28.312649Z"
    },
    "id": "gL6JiLXkSGPI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(model):\n",
    "  with tempfile.NamedTemporaryFile(suffix=\".h5\") as temp_file:  \n",
    "    model.save(temp_file.name)\n",
    "\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "      f.write(temp_file.name)\n",
    "    \n",
    "    # print(f\"Zipped model is saved at: {zipped_file}\")\n",
    "\n",
    "    x = os.path.getsize(zipped_file)\n",
    "    \n",
    "    os.remove(zipped_file)\n",
    "    # print(f\"Temporary zip file removed: {zipped_file}\")\n",
    "        \n",
    "    return x / 1000\n",
    "\n",
    "\n",
    "def print_model_weights_sparsity(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )\n",
    "\n",
    "\n",
    "def get_gzipped_model_size2(file):\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)/1000\n",
    "\n",
    "\n",
    "def eval_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy\n",
    "\n",
    "model_acc = []\n",
    "model_sz = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKzOfl5FSGPL"
   },
   "source": [
    "## Creating a Base Model - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:49:28.318406Z",
     "iopub.status.busy": "2024-03-09T12:49:28.317612Z",
     "iopub.status.idle": "2024-03-09T12:50:51.327361Z",
     "shell.execute_reply": "2024-03-09T12:50:51.326512Z"
    },
    "id": "w7Fd6jZ7SGPL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2182 - accuracy: 0.9339 - val_loss: 0.0925 - val_accuracy: 0.9717\n",
      "Epoch 2/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0889 - accuracy: 0.9724 - val_loss: 0.0987 - val_accuracy: 0.9703\n",
      "Epoch 3/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0593 - accuracy: 0.9808 - val_loss: 0.0808 - val_accuracy: 0.9763\n",
      "Epoch 4/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0455 - accuracy: 0.9854 - val_loss: 0.0658 - val_accuracy: 0.9808\n",
      "Epoch 5/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0710 - val_accuracy: 0.9798\n",
      "Epoch 6/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.0870 - val_accuracy: 0.9768\n",
      "Epoch 7/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1021 - val_accuracy: 0.9783\n",
      "Epoch 8/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0214 - accuracy: 0.9929 - val_loss: 0.0797 - val_accuracy: 0.9825\n",
      "Epoch 9/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 0.0849 - val_accuracy: 0.9833\n",
      "Epoch 10/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.1072 - val_accuracy: 0.9805\n",
      "Epoch 11/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.1034 - val_accuracy: 0.9803\n",
      "Epoch 12/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.1063 - val_accuracy: 0.9798\n",
      "Epoch 13/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.0988 - val_accuracy: 0.9835\n",
      "Epoch 14/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.1063 - val_accuracy: 0.9790\n",
      "Epoch 15/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.1082 - val_accuracy: 0.9805\n",
      "Epoch 16/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.1311 - val_accuracy: 0.9773\n",
      "Epoch 17/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.1110 - val_accuracy: 0.9825\n",
      "Epoch 18/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.1162 - val_accuracy: 0.9807\n",
      "Epoch 19/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0897 - val_accuracy: 0.9853\n",
      "Epoch 20/25\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.1516 - val_accuracy: 0.9760\n",
      "Epoch 21/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1181 - val_accuracy: 0.9820\n",
      "Epoch 22/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.1132 - val_accuracy: 0.9838\n",
      "Epoch 23/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.1126 - val_accuracy: 0.9823\n",
      "Epoch 24/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.1097 - val_accuracy: 0.9848\n",
      "Epoch 25/25\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.1277 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x28e836050>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images  = test_images / 255.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  keras.layers.Dense(300, activation='relu'),\n",
    "  keras.layers.Dense(100, activation='relu'),\n",
    "  keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer=opt,\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_split=0.1,\n",
    "    epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBOQ8MeESGPO"
   },
   "source": [
    "### Evaluate the baseline model and save it for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:51.331343Z",
     "iopub.status.busy": "2024-03-09T12:50:51.330742Z",
     "iopub.status.idle": "2024-03-09T12:50:51.987721Z",
     "shell.execute_reply": "2024-03-09T12:50:51.986958Z"
    },
    "id": "HYulekocSGPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9804999828338623\n",
      "Base model size:  2868.061  KB\n",
      "Top-1 Error = 1.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "sz = get_gzipped_model_size(model)\n",
    "print(\"Base model size: \",  sz , ' KB' )\n",
    "\n",
    "model_acc.append(baseline_model_accuracy)\n",
    "model_sz.append(sz)\n",
    "\n",
    "print(f\"Top-1 Error = {(1-baseline_model_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ---------- Checkpoint Point 1 ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPoCr4OFkXZE"
   },
   "source": [
    "## Pruning and then fine-tuning the model - 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:51.992256Z",
     "iopub.status.busy": "2024-03-09T12:50:51.991693Z",
     "iopub.status.idle": "2024-03-09T12:50:52.943566Z",
     "shell.execute_reply": "2024-03-09T12:50:52.942753Z"
    },
    "id": "mqsN5tP-kXZF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0989 - accuracy: 0.9660 - val_loss: 0.1357 - val_accuracy: 0.9613\n",
      "Epoch 2/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0619 - accuracy: 0.9809 - val_loss: 0.1110 - val_accuracy: 0.9712\n",
      "Epoch 3/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0435 - accuracy: 0.9877 - val_loss: 0.0992 - val_accuracy: 0.9753\n",
      "Epoch 4/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 0.0924 - val_accuracy: 0.9767\n",
      "Epoch 5/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0280 - accuracy: 0.9929 - val_loss: 0.0882 - val_accuracy: 0.9775\n",
      "Epoch 6/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0241 - accuracy: 0.9940 - val_loss: 0.0852 - val_accuracy: 0.9792\n",
      "Epoch 7/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 0.0829 - val_accuracy: 0.9793\n",
      "Epoch 8/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.0812 - val_accuracy: 0.9802\n",
      "Epoch 9/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0171 - accuracy: 0.9959 - val_loss: 0.0798 - val_accuracy: 0.9802\n",
      "Epoch 10/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.0786 - val_accuracy: 0.9803\n",
      "Epoch 11/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0142 - accuracy: 0.9967 - val_loss: 0.0776 - val_accuracy: 0.9808\n",
      "Epoch 12/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.0767 - val_accuracy: 0.9810\n",
      "Epoch 13/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0121 - accuracy: 0.9972 - val_loss: 0.0760 - val_accuracy: 0.9812\n",
      "Epoch 14/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0112 - accuracy: 0.9974 - val_loss: 0.0754 - val_accuracy: 0.9820\n",
      "Epoch 15/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.0748 - val_accuracy: 0.9820\n",
      "Epoch 16/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 0.0743 - val_accuracy: 0.9822\n",
      "Epoch 17/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 0.0739 - val_accuracy: 0.9827\n",
      "Epoch 18/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.0735 - val_accuracy: 0.9828\n",
      "Epoch 19/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0732 - val_accuracy: 0.9828\n",
      "Epoch 20/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.0730 - val_accuracy: 0.9828\n",
      "Epoch 21/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0728 - val_accuracy: 0.9828\n",
      "Epoch 22/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.0726 - val_accuracy: 0.9830\n",
      "Epoch 23/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.0724 - val_accuracy: 0.9833\n",
      "Epoch 24/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0723 - val_accuracy: 0.9832\n",
      "Epoch 25/25\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.0722 - val_accuracy: 0.9835\n",
      "dense_53/kernel:0: 70.00% sparsity  (164640/235200)\n",
      "dense_54/kernel:0: 70.00% sparsity  (21000/30000)\n",
      "dense_55/kernel:0: 70.00% sparsity  (700/1000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.7, begin_step=0, frequency=100)     \n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "    \n",
    "    \n",
    "pruned_model = model\n",
    "\n",
    "for i in range(1):\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    pruned_model = prune_low_magnitude(pruned_model, **pruning_params)\n",
    "\n",
    "    # learning rate for fine-tuning\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    \n",
    "    \n",
    "    pruned_model.compile(\n",
    "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=opt,\n",
    "      metrics=['accuracy'])\n",
    "    \n",
    "    # Fine-tune model\n",
    "    pruned_model.fit(\n",
    "      train_images,\n",
    "      train_labels,\n",
    "      epochs=25,\n",
    "      validation_split=0.1,\n",
    "      callbacks=callbacks)\n",
    "    \n",
    "    stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "    print_model_weights_sparsity(stripped_pruned_model)\n",
    "    \n",
    "    pruned_model = stripped_pruned_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:50:52.948105Z",
     "iopub.status.busy": "2024-03-09T12:50:52.947434Z",
     "iopub.status.idle": "2024-03-09T12:51:17.402820Z",
     "shell.execute_reply": "2024-03-09T12:51:17.401918Z"
    },
    "id": "2aBxR8uEkXZG"
   },
   "outputs": [],
   "source": [
    "pruned_model.compile(\n",
    "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=opt,\n",
    "      metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:17.416380Z",
     "iopub.status.busy": "2024-03-09T12:51:17.415788Z",
     "iopub.status.idle": "2024-03-09T12:51:17.453872Z",
     "shell.execute_reply": "2024-03-09T12:51:17.453121Z"
    },
    "id": "_8_p--1NkXZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model test accuracy: 0.9824000000953674\n",
      "Stripped model size:  2158.811  KB\n"
     ]
    }
   ],
   "source": [
    "_, pruned_model_accuracy = pruned_model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Pruned Model test accuracy:', pruned_model_accuracy)\n",
    "\n",
    "sz = get_gzipped_model_size(pruned_model)\n",
    "print(\"Stripped model size: \",  sz , ' KB' )\n",
    "\n",
    "model_acc.append(pruned_model_accuracy)\n",
    "model_sz.append(sz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ---------- Checkpoint Point 2 ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation - 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import ops\n",
    "import numpy as np\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            ops.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            ops.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2)\n",
    "\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "teacher_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "        keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu),  \n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu), \n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units=256, activation=tf.nn.relu), \n",
    "        keras.layers.Dense(units=128, activation=tf.nn.relu), \n",
    "        keras.layers.Dense(10)\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# teacher = keras.Sequential(\n",
    "#     [\n",
    "#           keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "#           keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "#           keras.layers.Conv2D(filters=12, kernel_size=(3, 3),\n",
    "#                                  activation=tf.nn.relu),\n",
    "#           keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#           keras.layers.Flatten(),\n",
    "#           keras.layers.Dense(10)\n",
    "#     ],\n",
    "#     name=\"teacher\",\n",
    "# )\n",
    "\n",
    "\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0429 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9986\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9988\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0039 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9989\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9995\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9993\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9994\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9992\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0015 - sparse_categorical_accuracy: 0.9995\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9994\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9994\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06758920848369598, 0.9915000200271606]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.fit(train_images, train_labels, epochs=20)\n",
    "teacher.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "distiller = Distiller(student=pruned_model, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 9s 4ms/step - sparse_categorical_accuracy: 0.9950\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9969\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9968\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9978\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9972\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9975\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9970\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9981\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9976\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9973\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9983\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9974\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - sparse_categorical_accuracy: 0.9974\n",
      "313/313 [==============================] - 0s 475us/step - sparse_categorical_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9814000129699707"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distill teacher se student\n",
    "distiller.fit(train_images, train_labels, epochs=15)\n",
    "\n",
    "distiller.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Model test accuracy: 0.9814000129699707\n",
      "Distilled model size:  2651.287  KB\n"
     ]
    }
   ],
   "source": [
    "_, acc = distiller.student.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Distilled Model test accuracy:', acc)\n",
    "\n",
    "sz = get_gzipped_model_size(distiller.student)\n",
    "print(\"Distilled model size: \",  sz , ' KB' )\n",
    "\n",
    "model_acc.append(acc)\n",
    "model_sz.append(sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ---------- Checkpoint Point 3 ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Clustering - 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_weight_clusters(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            # ignore auxiliary quantization weights\n",
    "            if \"quantize_layer\" in weight.name:\n",
    "                continue\n",
    "            if \"kernel\" in weight.name:\n",
    "                unique_count = len(np.unique(weight))\n",
    "                print(\n",
    "                    f\"{layer.name}/{weight.name}: {unique_count} clusters \"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:17.457801Z",
     "iopub.status.busy": "2024-03-09T12:51:17.457165Z",
     "iopub.status.idle": "2024-03-09T12:51:42.585401Z",
     "shell.execute_reply": "2024-03-09T12:51:42.584660Z"
    },
    "id": "RetnGeQnkXZH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sparsity preserving clustering model:\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 11s 6ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0074 - val_accuracy: 0.9970\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0171 - val_accuracy: 0.9948\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0134 - val_accuracy: 0.9955\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0134 - val_accuracy: 0.9962\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0175 - val_accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0165 - val_accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0229 - val_accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0175 - val_accuracy: 0.9955\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0273 - val_accuracy: 0.9930\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0218 - val_accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x28c7b2cd0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.clustering.keras.experimental import (\n",
    "    cluster,\n",
    ")\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "cluster_weights = cluster.cluster_weights\n",
    "\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 8,\n",
    "  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS,\n",
    "  'preserve_sparsity': True\n",
    "}\n",
    "\n",
    "sparsity_clustered_model = cluster_weights(distiller.student, **clustering_params)\n",
    "\n",
    "sparsity_clustered_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train sparsity preserving clustering model:')\n",
    "sparsity_clustered_model.fit(train_images, train_labels,epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:42.589384Z",
     "iopub.status.busy": "2024-03-09T12:51:42.588809Z",
     "iopub.status.idle": "2024-03-09T12:51:42.647457Z",
     "shell.execute_reply": "2024-03-09T12:51:42.646687Z"
    },
    "id": "iHN3NW8OkXZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sparsity:\n",
      "\n",
      "kernel:0: 38.37% sparsity  (90252/235200)\n",
      "kernel:0: 29.65% sparsity  (8896/30000)\n",
      "kernel:0: 9.40% sparsity  (94/1000)\n",
      "\n",
      "Model clusters:\n",
      "\n",
      "dense_53/kernel:0: 8 clusters \n",
      "dense_54/kernel:0: 8 clusters \n",
      "dense_55/kernel:0: 8 clusters \n"
     ]
    }
   ],
   "source": [
    "stripped_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\n",
    "\n",
    "print(\"Model sparsity:\\n\")\n",
    "print_model_weights_sparsity(stripped_clustered_model)\n",
    "\n",
    "print(\"\\nModel clusters:\\n\")\n",
    "print_model_weight_clusters(stripped_clustered_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_clustered_model.compile(optimizer=opt,\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered Model test accuracy: 0.9835000038146973\n",
      "Clustered model size:  1869.878  KB\n"
     ]
    }
   ],
   "source": [
    "_, stripped_clustered_model_accuracy = stripped_clustered_model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Clustered Model test accuracy:', stripped_clustered_model_accuracy)\n",
    "\n",
    "sz = get_gzipped_model_size(stripped_clustered_model)\n",
    "print(\"Clustered model size: \",  sz , ' KB' )\n",
    "\n",
    "model_acc.append(stripped_clustered_model_accuracy)\n",
    "model_sz.append(sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ---------- Checkpoint Point 4 ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization - 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:42.651439Z",
     "iopub.status.busy": "2024-03-09T12:51:42.650879Z",
     "iopub.status.idle": "2024-03-09T12:51:55.132299Z",
     "shell.execute_reply": "2024-03-09T12:51:55.131490Z"
    },
    "id": "Nfp-xfHdZIUc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training after quantization model:\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_53/kernel:0', 'dense_54/kernel:0', 'dense_55/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_53/kernel:0', 'dense_54/kernel:0', 'dense_55/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_53/kernel:0', 'dense_54/kernel:0', 'dense_55/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_53/kernel:0', 'dense_54/kernel:0', 'dense_55/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 4s 7ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0137 - val_accuracy: 0.9967\n",
      "Epoch 2/3\n",
      "422/422 [==============================] - 3s 7ms/step - loss: 1.0853e-04 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 0.9965\n",
      "Epoch 3/3\n",
      "422/422 [==============================] - 3s 7ms/step - loss: 3.5252e-05 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x289b3d450>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              stripped_clustered_model)\n",
    "quant_model = tfmot.quantization.keras.quantize_apply(\n",
    "              quant_aware_annotate_model,\n",
    "              tfmot.experimental.combine.Default8BitClusterPreserveQuantizeScheme(preserve_sparsity=True))\n",
    "\n",
    "quant_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "print('Training after quantization model:')\n",
    "quant_model.fit(train_images, train_labels, batch_size=128, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:51:55.136081Z",
     "iopub.status.busy": "2024-03-09T12:51:55.135494Z",
     "iopub.status.idle": "2024-03-09T12:51:55.145936Z",
     "shell.execute_reply": "2024-03-09T12:51:55.145243Z"
    },
    "id": "6kluyg_2ZIUd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model clusters:\n",
      "quant_dense_53/dense_53/kernel:0: 8 clusters \n",
      "quant_dense_54/dense_54/kernel:0: 8 clusters \n",
      "quant_dense_55/dense_55/kernel:0: 8 clusters \n",
      "\n",
      "Final Model sparsity:\n",
      "dense_53/kernel:0: 44.45% sparsity  (104548/235200)\n",
      "dense_54/kernel:0: 32.10% sparsity  (9631/30000)\n",
      "dense_55/kernel:0: 11.30% sparsity  (113/1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Model clusters:\")\n",
    "print_model_weight_clusters(quant_model)\n",
    "print(\"\\nFinal Model sparsity:\")\n",
    "print_model_weights_sparsity(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmph494cps9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmph494cps9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model size:  87.793  KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaymaheshwari/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1717007315.296752  103872 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1717007315.297382  103872 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-29 23:58:35.298068: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmph494cps9\n",
      "2024-05-29 23:58:35.302246: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-29 23:58:35.302257: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmph494cps9\n",
      "2024-05-29 23:58:35.315366: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-29 23:58:35.362090: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/px/z8lb6znd6q95tq6vlznyb0s40000gn/T/tmph494cps9\n",
      "2024-05-29 23:58:35.373598: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 75532 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "final_tflite_model = converter.convert()\n",
    "final_model_file = 'final_model.tflite'\n",
    "# Save the model.\n",
    "with open(final_model_file, 'wb') as f:\n",
    "    f.write(final_tflite_model)\n",
    "\n",
    "\n",
    "sz = get_gzipped_model_size2(final_model_file)\n",
    "print(\"Final model size: \", sz, ' KB')\n",
    "\n",
    "model_sz.append(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:52:00.353105Z",
     "iopub.status.busy": "2024-03-09T12:52:00.352520Z",
     "iopub.status.idle": "2024-03-09T12:52:01.104976Z",
     "shell.execute_reply": "2024-03-09T12:52:01.104182Z"
    },
    "id": "6p4RBECpZIUg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(final_model_file)\n",
    "interpreter.allocate_tensors()\n",
    " \n",
    "final_test_accuracy = eval_model(interpreter)\n",
    "\n",
    "print('Final test accuracy:', final_test_accuracy)\n",
    "\n",
    "model_acc.append(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ---------- Checkpoint Point 5 ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 98.05  |  size = 2868.061 KB  |  Top-1 Error = 1.95%\n",
      "Accuracy = 98.24  |  size = 2158.811 KB  |  Top-1 Error = 1.76%\n",
      "Accuracy = 98.14  |  size = 2651.287 KB  |  Top-1 Error = 1.86%\n",
      "Accuracy = 98.35  |  size = 1869.878 KB  |  Top-1 Error = 1.65%\n",
      "Accuracy = 98.6  |  size = 87.793 KB  |  Top-1 Error = 1.40%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_acc)):\n",
    "    print(f\"Accuracy = { round(model_acc[i]*100,2)}  |  size = {model_sz[i]} KB  |  Top-1 Error = {(1-model_acc[i])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ------- Final Comparison Summary -------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pcqat_example.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
